#!/usr/bin/env python3
import os
import orjson
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed


def process_single_summary(summary_path, description):
    if not summary_path.exists():
        return {"description": description, "iptm": None, "ptm_A": None, "status": "missing"}

    try:
        with open(summary_path, "rb") as f:
            summary = orjson.loads(f.read())

        iptm = summary.get("iptm", None)
        chain_ptm = summary.get("chain_ptm", [])
        if not isinstance(chain_ptm, list):
            chain_ptm = []

        return {
            "description": description,
            "iptm": iptm,
            "ptm_A": chain_ptm[0] if chain_ptm else None,
            "status": "ok"
        }
    except Exception as e:
        return {"description": description, "iptm": None, "ptm_A": None, "status": f"error: {e}"}


def extract_summary_metrics(base_dir, sample_dir="seed-10_sample-0",
                            num_workers=64, batch_size=10000,
                            save_csv="results.csv", resume=False):
    descriptions = [d for d in os.listdir(base_dir) if (Path(base_dir) / d).is_dir()]
    paths = [(Path(base_dir) / d / sample_dir / "summary_confidences.json", d) for d in descriptions]

    processed = set()
    if resume and Path(save_csv).exists():
        try:
            df_existing = pd.read_csv(save_csv, usecols=["description"])
            processed = set(df_existing["description"].astype(str))
            print(f"ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰ {len(processed)} æ¡è®°å½•ï¼Œå°†è·³è¿‡è¿™äº›ä»»åŠ¡")
        except Exception:
            print("âš ï¸ å·²å­˜åœ¨çš„ CSV è¯»å–å¤±è´¥ï¼Œæ— æ³•æ–­ç‚¹ç»­è·‘ï¼Œå°†é‡æ–°å¼€å§‹")
            os.remove(save_csv)

    elif not resume and Path(save_csv).exists():
        print("âš ï¸ æœªå¯ç”¨ --resumeï¼Œæ—§ç»“æœå°†è¢«è¦†ç›–")
        os.remove(save_csv)

    total = len(paths)
    for i in range(0, total, batch_size):
        batch = paths[i:i+batch_size]
        batch = [(p, d) for p, d in batch if d not in processed]
        if not batch:
            print(f"âœ… è·³è¿‡æ‰¹æ¬¡ {i//batch_size+1}ï¼ˆå·²å¤„ç†è¿‡ï¼‰")
            continue

        print(f"\nğŸš€ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {i//batch_size+1}ï¼ŒåŒ…å« {len(batch)} ä¸ªæ–‡ä»¶...")

        results = []
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = {executor.submit(process_single_summary, p, d): d for p, d in batch}
            for j, fut in enumerate(tqdm(as_completed(futures), total=len(batch), desc=f"Batch {i//batch_size+1}")):
                res = fut.result()
                results.append(res)
                if (j + 1) % 1000 == 0:
                    print(f"   ğŸ”¹ å·²å®Œæˆ {j+1}/{len(batch)} ä¸ªæ–‡ä»¶")

        df_batch = pd.DataFrame(results)
        df_batch.to_csv(save_csv, mode="a", header=(not Path(save_csv).exists()), index=False)

        print(f"âœ… æ‰¹æ¬¡ {i//batch_size+1} å¤„ç†å®Œæˆï¼Œæœ¬æ‰¹ä¿å­˜ {len(df_batch)} æ¡è®°å½•")

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼Œå…± {total} é¡¹ä»»åŠ¡ï¼Œç»“æœä¿å­˜åˆ° {save_csv}")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="æ‰¹é‡æå– AF3 summary_confidences.json çš„ iptm / ptm ä¿¡æ¯")
    parser.add_argument("af3score_output_dir", help="AF3 ç»“æœçš„ä¸»ç›®å½•")
    parser.add_argument("save_csv", help="è¾“å‡º CSV è·¯å¾„")
    parser.add_argument("--sample_dir", type=str, default="seed-10_sample-0", help="å­ç›®å½•åï¼Œä¾‹å¦‚ seed-10_sample-0")
    parser.add_argument("--num_workers", type=int, default=64, help="å¹¶è¡Œçº¿ç¨‹æ•°")
    parser.add_argument("--batch_size", type=int, default=5000, help="æ¯æ‰¹æ¬¡å¤„ç†çš„æ–‡ä»¶æ•°é‡")
    parser.add_argument("--resume", action="store_true", help="æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­è·‘")
    args = parser.parse_args()

    extract_summary_metrics(
        args.af3score_output_dir,
        sample_dir=args.sample_dir,
        num_workers=args.num_workers,
        batch_size=args.batch_size,
        save_csv=args.save_csv,
        resume=args.resume
    )
