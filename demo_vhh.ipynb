{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d022f33c",
   "metadata": {},
   "source": [
    "# VHH Design\n",
    "\n",
    "### Case Study: Targeting CCL2 (Chemokine Ligand 2)\n",
    "\n",
    "This notebook demonstrates the end-to-end computational pipeline for designing VHH (Nanobody) binders. We use a combination of deep learning-based backbone generation, sequence design, and flow-based affinity maturation.\n",
    "\n",
    "---\n",
    "\n",
    "## Design Pipeline\n",
    "\n",
    "The workflow is organized into four key modules:\n",
    "\n",
    "### Hotspot Identification\n",
    "* **Objective**: Define the binding epitope on the **CCL2** surface.\n",
    "* **Selection Criteria**: \n",
    "    * Focused on specific **hotspots** critical for protein-protein interactions (PPI).\n",
    "    * Preference for **hydrophobic patches** to maximize surface complementarity.\n",
    "\n",
    "### Step.1 Backbone Generation & Sequence Design\n",
    "* **Backbone**: Sampling diverse CDR configurations that fit the target epitope using **PPIFlow**.\n",
    "* **Sequence**: Inverse folding and amino acid sequence optimization using **AbMPNN** .\n",
    "\n",
    "### Step.2 Scoring & Filtering\n",
    "* **Validation**: High-throughput screening of designs using **FlowPacker** and **AF3Score**.\n",
    "* **Metrics**: Candidates are filtered based on:\n",
    "    * **$iPTM$**\n",
    "    * **$PTM$**\n",
    "### Step.3 Rosetta interface \n",
    "### Step.4 Affinity Maturation (PPIFlow)\n",
    "* **Refinement**: Top-tier candidates undergo maturation using **PPIFlow (Partial Flow version)**.\n",
    "* **Optimization**: Leveraging discrete-continuous flow matching to fine-tune the interface residues for sub-nanomolar affinity.\n",
    "\n",
    "### Step.5 Scoring & Filtering\n",
    "### Step.6 Rosetta relax \n",
    "### Step.7 AF3 refold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be7632",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir='.'\n",
    "demo_scripts='demo_scripts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3f4cf",
   "metadata": {},
   "source": [
    "# Stes.1.1 Backbone Generation\n",
    "\n",
    "Run the following command in the ppiflow environment on a compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21caaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "def submit_nanobody_job(\n",
    "    antigen_pdb: str,\n",
    "    framework_pdb: str,\n",
    "    specified_hotspots: str,\n",
    "    samples_per_target: int,\n",
    "    output_dir: str,\n",
    "    job_name: str = \"PPIFlow_nanobody\",\n",
    "    gpu_partition: str = \"gpu41,gpu43,gpu53,gpu51\",\n",
    "    gpus: int = 1,\n",
    "    cpus: int = 4,\n",
    "    model_weights: str = \"path_to_ckpt\",\n",
    "    config: str = \"configs/inference_nanobody.yaml\",\n",
    "    antigen_chain: str = \"C\",\n",
    "    heavy_chain: str = \"A\",\n",
    "    light_chain: str = \"B\",\n",
    "    cdr_length: str = \"CDRH1,8-8,CDRH2,8-8,CDRH3,9-21\",\n",
    "    conda_python: str = \"miniconda3/envs/ppiflow/bin/python\",\n",
    "    log_dir: str = \"logs\",\n",
    "):\n",
    "    \"\"\"Generate SLURM shell script for nanobody backbone generation and submit it.\"\"\"\n",
    "    \n",
    "    Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
    "    Path(output_dir).mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    sh_content = f\"\"\"#!/bin/bash\n",
    "#SBATCH -p {gpu_partition}\n",
    "#SBATCH --gres=gpu:{gpus}\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task={cpus}\n",
    "#SBATCH -N 1\n",
    "#SBATCH -o {log_dir}/%j_%x.out\n",
    "#SBATCH -J {job_name}\n",
    "export MPLBACKEND=Agg\n",
    "\n",
    "echo \"Job started at: $(date)\"\n",
    "start_time=$(date +%s)\n",
    "\n",
    "ANTIGEN_PDB=\"{antigen_pdb}\"\n",
    "FRAMEWORK_PDB=\"{framework_pdb}\"\n",
    "SPECIFIED_HOTSPOTS=\"{specified_hotspots}\"\n",
    "CDR_LENGTH=\"{cdr_length}\"\n",
    "SAMPLES_PER_TARGET=\"{samples_per_target}\"\n",
    "MODEL_WEIGHTS=\"{model_weights}\"\n",
    "OUTPUT_DIR=\"{output_dir}\"\n",
    "NAME=$(basename $OUTPUT_DIR)\n",
    "\n",
    "ANTIGEN_CHAIN=\"{antigen_chain}\"\n",
    "HEAVY_CHAIN=\"{heavy_chain}\"\n",
    "LIGHT_CHAIN=\"{light_chain}\"\n",
    "CONFIG=\"{config}\"\n",
    "\n",
    "{conda_python} sample_antibody_nanobody.py \\\\\n",
    "    --antigen_pdb $ANTIGEN_PDB \\\\\n",
    "    --framework_pdb $FRAMEWORK_PDB \\\\\n",
    "    --antigen_chain $ANTIGEN_CHAIN \\\\\n",
    "    --heavy_chain $HEAVY_CHAIN \\\\\n",
    "    --specified_hotspots $SPECIFIED_HOTSPOTS \\\\\n",
    "    --cdr_length $CDR_LENGTH \\\\\n",
    "    --samples_per_target $SAMPLES_PER_TARGET \\\\\n",
    "    --config $CONFIG \\\\\n",
    "    --model_weights $MODEL_WEIGHTS \\\\\n",
    "    --output_dir $OUTPUT_DIR \\\\\n",
    "    --name $NAME\n",
    "\n",
    "end_time=$(date +%s)\n",
    "echo \"Job ended at: $(date)\"\n",
    "elapsed=$((end_time - start_time))\n",
    "echo \"Elapsed time: $elapsed seconds\"\n",
    "\"\"\"\n",
    "\n",
    "    sh_path = Path(output_dir) / f\"{job_name}.sh\"\n",
    "    with open(sh_path, \"w\") as f:\n",
    "        f.write(sh_content)\n",
    "    \n",
    "    print(f\"SLURM script written to {sh_path}\")\n",
    "\n",
    "    subprocess.run([\"sbatch\", str(sh_path)])\n",
    "    print(f\"Job submitted with sbatch.\")\n",
    "\n",
    "\n",
    "submit_nanobody_job(\n",
    "    antigen_pdb=f\"{project_dir}/test/input/4dn4.pdb\",\n",
    "    framework_pdb=f\"{project_dir}/test/input/7xl0_nanobody_framework.pdb\",\n",
    "    antigen_chain='M',\n",
    "    specified_hotspots=\"M28,M39,M55,M61\",\n",
    "    samples_per_target=10,\n",
    "    output_dir=f\"{project_dir}/test/4dn4_ccl2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee3b16",
   "metadata": {},
   "source": [
    "If you are working on a compute node, you can directly run the Python code below.\n",
    "\n",
    "```\n",
    "python sample_antibody_nanobody.py \\\n",
    "    --antigen_pdb path_to_input/4dn4.pdb \\\n",
    "    --framework_pdb path_to_input/7xl0_nanobody_framework.pdb \\\n",
    "    --antigen_chain M \\\n",
    "    --heavy_chain A \\\n",
    "    --specified_hotspots \"M28,M39,M55,M61\" \\\n",
    "    --cdr_length \"CDRH1,8-8,CDRH2,8-8,CDRH3,9-21\" \\\n",
    "    --samples_per_target 10 \\\n",
    "    --config configs/inference_nanobody.yaml \\\n",
    "    --model_weights path_to_ckpt/epoch=122-step=25461.ckpt \\\n",
    "    --output_dir path_to_test/nanobody_test/4dn4_ccl2 \\\n",
    "    --name 4dn4_ccl2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848aea9b",
   "metadata": {},
   "source": [
    "## Arguments\n",
    "\n",
    "To avoid potential runtime errors, all file paths should be specified as **absolute paths**.\n",
    "\n",
    "- `--antigen_pdb`\n",
    "Path to the antigen PDB structure.\n",
    "\n",
    "- `--framework_pdb`\n",
    "Input nanobody framework structure.\n",
    "\n",
    "The framework should have all three CDR regions removed according to the `IMGT` numbering scheme.\n",
    "\n",
    "If you do not have a custom framework, you may use the one provided at `./Framework/`.\n",
    "\n",
    "- `--antigen_chain`\n",
    "Chain ID of the antigen in the PDB file.\n",
    "\n",
    "- `--heavy_chain`\n",
    "Chain ID of the nanobody (heavy chain).\n",
    "\n",
    "- `--specified_hotspots`\n",
    "\n",
    "Antigen hotspot residues used to guide backbone generation.\n",
    "\n",
    "Residue indices must be consistent with the numbering in the input structure.\n",
    "\n",
    "- `--cdr_length`\n",
    "Length ranges for the three CDRs (CDRH1, CDRH2, CDRH3).\n",
    "\n",
    "- `--samples_per_target`\n",
    "Number of backbone conformations sampled for each target.\n",
    "\n",
    "- `--config`\n",
    "Path to the inference configuration file.\n",
    "\n",
    "- `--model_weights`\n",
    "Path to the pretrained model checkpoint.\n",
    "\n",
    "- `--output_dir`\n",
    "Directory for saving generated backbone structures.\n",
    "\n",
    "- `--name`\n",
    "Prefix used for naming the generated structure files\n",
    "(e.g., `4dn4_ccl2_0.pdb`).\n",
    "\n",
    "Lowercase characters are recommended to ensure consistency with\n",
    "downstream **AF3Score** processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdr interface ratio >= 0.6\n",
    "import pandas as pd\n",
    "import os,shutil\n",
    "df=pd.read_csv(f'{project_dir}/test/4dn4_ccl2/sample_metrics.csv')\n",
    "df_f=df[df['cdr_interface_ratio']>=0.6]\n",
    "print(df_f.shape)\n",
    "\n",
    "out_dir = f'{project_dir}/test/4dn4_ccl2/filtered_links'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "df_f.apply(lambda row: shutil.copy(os.path.abspath(row['pdb_path']), \n",
    "                                  os.path.join(out_dir, row['sample'])), axis=1)\n",
    "\n",
    "print(f\"Linked {len(df_f)} files to {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65288510",
   "metadata": {},
   "source": [
    "# Step.1.2 Sequence design\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python $demo_scripts/make_fix_csv.py test/4dn4_ccl2/filtered_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682f4e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2910054\n"
     ]
    }
   ],
   "source": [
    "!sbatch $demo_scripts/run_abmpnn.sh test/4dn4_ccl2/filtered_links test/4dn4_ccl2_fa test/4dn4_ccl2/filtered_links/fixed_positions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca5681",
   "metadata": {},
   "source": [
    "# Step.2 Scoring & Filtering\n",
    "The code repository will be updated in subsequent releases. \n",
    "\n",
    "For now, we briefly outline the overall logic of the pipeline: \n",
    "\n",
    "FlowPacker is first used to add side chains onto the backbones designed by PPIFlow, and the resulting full-atom structures are then evaluated using AF3Score for structural and interface quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from Bio import SeqIO\n",
    "\n",
    "def direct_fasta_to_csv(input_dirs: list, output_csv: str, suffix: str = \".pdb\"):\n",
    "\n",
    "    seen_seqs = set()  \n",
    "\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(output_csv)), exist_ok=True)\n",
    "\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"link_name\", \"seq\", \"seq_idx\"]) \n",
    "        for folder in input_dirs:\n",
    "            if not os.path.exists(folder): continue\n",
    "            files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(('.fasta', '.fa'))]\n",
    "\n",
    "            for file_path in files:\n",
    "                base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "                for i, record in enumerate(SeqIO.parse(file_path, \"fasta\")):\n",
    "                    if i == 0: \n",
    "                        continue \n",
    "\n",
    "                    seq_str = str(record.seq)\n",
    "                    if seq_str in seen_seqs: \n",
    "                        continue \n",
    "                    seen_seqs.add(seq_str)\n",
    "                    link_name = f\"{base_name}{suffix}\"\n",
    "                    seq_idx = str(i)\n",
    "\n",
    "                    writer.writerow([link_name, seq_str, seq_idx])\n",
    "\n",
    "    print(f\"✅ Processing complete! {len(seen_seqs)} unique sequences have been written to: {output_csv}\")\n",
    "\n",
    "\n",
    "\n",
    "fa_folders = [\n",
    "    f'{project_dir}/test/4dn4_ccl2_fa/seqs',\n",
    "]\n",
    "\n",
    "direct_fasta_to_csv(\n",
    "    input_dirs=fa_folders, \n",
    "    output_csv=f'{project_dir}/test/4dn4_ccl2_fa/final_result.csv', \n",
    "    suffix=\".pdb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d5d63",
   "metadata": {},
   "source": [
    "## run in a tmux terminal\n",
    "```\n",
    "bash demo_scripts/flowpacker_af3score.sh\\\n",
    "    test/4dn4_ccl2/filtered_links \\\n",
    "    test/4dn4_ccl2_fa/final_result.csv \\\n",
    "    test/af3score \\\n",
    "    1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4489dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os,shutil\n",
    "df=pd.read_csv(f'{project_dir}/test/af3score/af3score_base_outputs/af3score_metrics.csv')\n",
    "df['pdb_path']='test/af3score/flowpacker/flowpacker_outputs/run_1/'+df['description']+'.pdb'\n",
    "df_f=df[df['iptm'] > 0.2]\n",
    "print(df_f.shape)\n",
    "\n",
    "out_dir = f'{project_dir}/test/af3score/filtered_links'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "df_f.apply(lambda row: os.link(os.path.abspath(row['pdb_path']), \n",
    "                                  os.path.join(out_dir, row['description']+'.pdb')), axis=1)\n",
    "\n",
    "print(f\"Linked {len(df_f)} files to {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ae7bd",
   "metadata": {},
   "source": [
    "# Step.3 Rosetta interface analysis\n",
    "\n",
    "```\n",
    "nohup bash demo_scripts/interface_analysis/0-run_rosetta.sh test/af3score/filtered_links test/iptm_filter_rst/ >> nohup.out &\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9df516ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pdb: 9\n",
      "start Pool\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]test/iptm_filter_rst/4dn4_ccl2_1_8.png\n",
      "test/iptm_filter_rst/4dn4_ccl2_0_7.png\n",
      "test/iptm_filter_rst/4dn4_ccl2_8_4.png\n",
      "test/iptm_filter_rst/4dn4_ccl2_0_4.png\n",
      "test/iptm_filter_rst/4dn4_ccl2_3_6.png\n",
      "test/iptm_filter_rst/4dn4_ccl2_0_5.png\n",
      "test/iptm_filter_rst/4dn4_ccl2_0_2.png\n",
      "test/iptm_filter_rst/4dn4_ccl2_1_4.png\n",
      "test/iptm_filter_rst/4dn4_ccl2_1_5.png\n",
      "  binder_id target_id binder_res target_res  total  in_interface\n",
      "0         A         M         52         22 -0.194          True\n",
      "1         A         M         53         20 -2.314          True\n",
      "Index(['binder_id', 'target_id', 'binder_res', 'target_res', 'total',\n",
      "       'in_interface'],\n",
      "      dtype='object')\n",
      "  binder_id target_id binder_res target_res  total  in_interface\n",
      "0         A         M          1         57 -0.141          True\n",
      "1         A         M          2         57 -0.249          True\n",
      "  binder_id target_id binder_res target_res  total  in_interface\n",
      "0         A         M         31         24 -1.336          True\n",
      "1         A         M         31         26 -0.566          True\n",
      "  binder_id target_id binder_res target_res  total  in_interface\n",
      "0         A         M         31         24 -1.350          True\n",
      "1         A         M         31         26 -0.718          True\n",
      "  binder_id target_id binder_res target_res  total  in_interface\n",
      "0         A         M          1         57 -0.203          True\n",
      "1         A         M          2         57 -0.478          True  binder_id target_id binder_res target_res  total  in_interface\n",
      "0         A         M          1         57 -0.089          True\n",
      "1         A         M          2         57 -0.284          True\n",
      "\n",
      "  binder_id target_id binder_res target_res  total  in_interface\n",
      "0         A         M          1         57 -0.349          True\n",
      "1         A         M          2         57 -0.447          True\n",
      "Index(['binder_id', 'target_id', 'binder_res', 'target_res', 'total',\n",
      "       'in_interface'],\n",
      "      dtype='object')\n",
      "Index(['binder_id', 'target_id', 'binder_res', 'target_res', 'total',\n",
      "       'in_interface'],\n",
      "      dtype='object')\n",
      "Index(['binder_id', 'target_id', 'binder_res', 'target_res', 'total',\n",
      "       'in_interface'],\n",
      "      dtype='object')\n",
      "Index(['binder_id', 'target_id', 'binder_res', 'target_res', 'total',\n",
      "       'in_interface'],\n",
      "      dtype='object')Index(['binder_id', 'target_id', 'binder_res', 'target_res', 'total',\n",
      "       'in_interface'],\n",
      "      dtype='object')\n",
      "\n",
      "Index(['binder_id', 'target_id', 'binder_res', 'target_res', 'total',\n",
      "       'in_interface'],\n",
      "      dtype='object')\n",
      "  binder_id target_id binder_res target_res  total  in_interface\n",
      "0         A         M         30         25 -0.002          True\n",
      "1         A         M         31         24 -1.310          True\n",
      "Index(['binder_id', 'target_id', 'binder_res', 'target_res', 'total',\n",
      "       'in_interface'],\n",
      "      dtype='object')\n",
      "  binder_id target_id binder_res target_res  total  in_interface\n",
      "0         A         M          2         60 -0.024          True\n",
      "1         A         M         27         60 -1.902          True\n",
      "Index(['binder_id', 'target_id', 'binder_res', 'target_res', 'total',\n",
      "       'in_interface'],\n",
      "      dtype='object')\n",
      "100%|█████████████████████████████████████████████| 9/9 [00:02<00:00,  3.32it/s]\n",
      "test/iptm_filter_rst\n",
      "Extracted values: 202\n",
      "Figure(1000x600)\n"
     ]
    }
   ],
   "source": [
    "!python demo_scripts/interface_analysis/get_interface_energy.py \\\n",
    "    --input_pdbdir test/af3score/filtered_links \\\n",
    "    --rosetta_dir test/iptm_filter_rst \\\n",
    "    --binder_id A \\\n",
    "    --target_id M \\\n",
    "    --output_dir test/iptm_filter_rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cda767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 8)\n"
     ]
    }
   ],
   "source": [
    "# extract v < -5\n",
    "import ast\n",
    "residue_energy = pd.read_csv(\"test/iptm_filter_rst/residue_energy.csv\")\n",
    "residue_energy[\"fixed_residue\"] = residue_energy[\"binder_energy\"].apply(lambda x: \"_\".join([str(k) for k, v in ast.literal_eval(x).items() if v < -5]))\n",
    "residue_energy[\"fixed_residue\"] = residue_energy[\"fixed_residue\"].astype(str)\n",
    "residue_energy[\"num_fixed_residue\"] = residue_energy[\"fixed_residue\"].apply(\n",
    "    lambda x: 0 if x == \"\" else len(x.split(\"_\"))\n",
    ")\n",
    "print(residue_energy.shape)\n",
    "residue_energy.to_csv('test/rst_intface_residue_energy_with_key_contacts.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea03783",
   "metadata": {},
   "source": [
    "# Step.4 Affinity maturation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fc22c",
   "metadata": {},
   "source": [
    "## Step.4.1 Parameter Preparation\n",
    "\n",
    "We need the **CDR regions**, the **framework**, and the **residue indices of key contacts** from the previous step with `v < -5`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab02dd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq saved at test/af3score/filtered_links.csv\n"
     ]
    }
   ],
   "source": [
    "!python demo_scripts/pdb2fa.py test/af3score/filtered_links csv test/af3score/filtered_links.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb06b506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6)\n"
     ]
    }
   ],
   "source": [
    "# extract \n",
    "BASE='test'\n",
    "residue_energy = pd.read_csv(f\"{BASE}/rst_intface_residue_energy_with_key_contacts.csv\")\n",
    "seq = f\"{BASE}/af3score/filtered_links.csv\"\n",
    "save_csv=f\"{BASE}/fixed_residue.csv\"\n",
    "\n",
    "seq_df_multi = pd.read_csv(seq)\n",
    "seq_df_multi['pdb_name']=seq_df_multi['pdb_name'].str.replace('.pdb','')\n",
    "residue_energy = residue_energy.rename(columns={'pdbname':'pdb_name'})\n",
    "\n",
    "residue_energy = pd.merge(residue_energy, seq_df_multi, on=\"pdb_name\")\n",
    "residue_energy[\"structure\"] = residue_energy[\"pdb_name\"].apply(lambda x: '_'.join(x.split(\"_\")[:-1]))\n",
    "residue_energy[\"num_fixed_residue\"] = residue_energy[\"fixed_residue\"].apply(lambda x: len(x.split(\"_\")) if not pd.isna(x) else 0)\n",
    "\n",
    "all_cand = []\n",
    "for i, group in residue_energy.groupby(\"structure\"):\n",
    "    key_res = {}\n",
    "    for j, row in group.iterrows():\n",
    "\n",
    "        inter_res = {str(k):[v, row[\"sequence\"][int(k)-1]] for k, v in ast.literal_eval(row[\"binder_energy\"]).items() if v < -5}\n",
    "\n",
    "        for k in inter_res:\n",
    "            if k not in key_res:\n",
    "                key_res[k] = inter_res[k]\n",
    "            elif inter_res[k][0] <= key_res[k][0]:\n",
    "                key_res[k] = inter_res[k]\n",
    "\n",
    "\n",
    "    all_cand.append([row[\"pdbpath\"], \"_\".join(row[\"pdb_name\"].split('_')[:-1]), row[\"target_id\"], row[\"binder_id\"], key_res, len(key_res)])\n",
    "\n",
    "        # key_res = {k: inter_res[k] if inter_res[k][0] <= key_res[k][0] else key_res[k] for k in inter_res}\n",
    "all_cand_df = pd.DataFrame(all_cand, columns=[\"pdb_path\", \"pdb_name\", \"target_id\", \"binder_id\", \"key_res\", \"num_fixed_residues\"])\n",
    "print(all_cand_df.shape)\n",
    "all_cand_df.to_csv(f'{save_csv}',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffca246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing: Renaming Chain A & Renumbering Chain M (start from 9)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# merge key residues into one pdb file\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_pdb_mutation_and_renumber(csv, pdb_output_dir, \n",
    "                                      renumber_chain='B',\n",
    "                                      start_index=1):\n",
    "    one_to_three = {\n",
    "        'A': 'ALA', 'R': 'ARG', 'N': 'ASN', 'D': 'ASP', 'C': 'CYS',\n",
    "        'Q': 'GLN', 'E': 'GLU', 'G': 'GLY', 'H': 'HIS', 'I': 'ILE',\n",
    "        'L': 'LEU', 'K': 'LYS', 'M': 'MET', 'F': 'PHE', 'P': 'PRO',\n",
    "        'S': 'SER', 'T': 'THR', 'W': 'TRP', 'Y': 'TYR', 'V': 'VAL'\n",
    "    }\n",
    "    \n",
    "    df = pd.read_csv(csv)\n",
    "    os.makedirs(pdb_output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Start processing: Renaming Chain A & Renumbering Chain {renumber_chain} (start from {start_index})...\")\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        \n",
    "        if not os.path.exists(row[\"pdb_path\"]):\n",
    "            print(f\"Warning: {row['pdb_path']} not found, skipping...\")\n",
    "            continue\n",
    "\n",
    "        resi_to_resname = {}\n",
    "        if \"key_res\" in row and pd.notna(row[\"key_res\"]):\n",
    "            try:\n",
    "                for resi, aa in ast.literal_eval(row[\"key_res\"]).items():\n",
    "                    resi_to_resname[int(resi)] = one_to_three.get(aa[1], \"UNK\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing key_res for {row['pdb_name']}: {e}\")\n",
    "\n",
    "        with open(row[\"pdb_path\"], \"r\") as f:\n",
    "            pdb_lines = f.readlines()\n",
    "\n",
    "        new_lines = []\n",
    "        \n",
    "        current_renumber_idx = start_index - 1\n",
    "        last_seen_resi_id = None\n",
    "\n",
    "        for line in pdb_lines:\n",
    "            if not line.startswith(\"ATOM\"):\n",
    "                new_lines.append(line)\n",
    "                continue\n",
    "\n",
    "            chain_id = line[21]\n",
    "            \n",
    "\n",
    "            if chain_id == \"A\":\n",
    "                resi = int(line[22:26])\n",
    "                if resi in resi_to_resname:\n",
    "                    new_resname = resi_to_resname[resi]\n",
    "\n",
    "                    line = line[:17] + new_resname.ljust(3) + line[20:]\n",
    "\n",
    "            elif chain_id == renumber_chain:\n",
    "                current_resi_id_str = line[22:27]                \n",
    "                if current_resi_id_str != last_seen_resi_id:\n",
    "                    current_renumber_idx += 1\n",
    "                    last_seen_resi_id = current_resi_id_str\n",
    "\n",
    "                new_resi_str = f\"{current_renumber_idx:>4}\"\n",
    "                line = line[:22] + new_resi_str + \" \" + line[27:]\n",
    "            new_lines.append(line)\n",
    "        output_pdb = os.path.join(pdb_output_dir, os.path.basename(row[\"pdb_name\"])+\".pdb\")\n",
    "        with open(output_pdb, \"w\") as f:\n",
    "            f.writelines(new_lines)\n",
    "    print(\"All done!\")\n",
    "\n",
    "process_pdb_mutation_and_renumber(\"test/fixed_residue.csv\", \"test/merge_motif_pdb/\", \n",
    "    renumber_chain='M', start_index=9) # The residue numbering of the target chain is consistent with the input target file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c283cf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing 4 files with 110 cores...\n",
      "Extracting Framework and CDR indices for Chain A...\n",
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00,  5.12pdb/s]\n",
      "\n",
      "Saved at test/filter_cdr_idx.csv\n"
     ]
    }
   ],
   "source": [
    "!python demo_scripts/get_cdr.py test/merge_motif_pdb test/filter_cdr_idx.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed and saved to test/fixed_residue_setting_for_partial_flow.csv\n",
      "                                r2_fixed_sequence_pm  \\\n",
      "0  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...   \n",
      "1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...   \n",
      "2  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...   \n",
      "3  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...   \n",
      "\n",
      "                                   r2_fixed_sequence  \n",
      "0  A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,A11,A12,A13,A14...  \n",
      "1  A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,A11,A12,A13,A14...  \n",
      "2  A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,A11,A12,A13,A14...  \n",
      "3  A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,A11,A12,A13,A14...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def safe_extract_keys(val):\n",
    "\n",
    "    if pd.isna(val) or str(val).strip() == \"\":\n",
    "        return \"\"\n",
    "    try:\n",
    "        d = ast.literal_eval(val)\n",
    "        if isinstance(d, dict):\n",
    "            return \" \".join([str(k).strip() for k in d.keys()])\n",
    "    except (ValueError, SyntaxError):\n",
    "        return \"\"\n",
    "    return \"\"\n",
    "\n",
    "def process_indices(val, output_format=\"space\"):\n",
    "\n",
    "    if pd.isna(val) or str(val).strip() == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        unique_sorted_ints = sorted(list(set(int(i) for i in str(val).split() if i.strip())))\n",
    "        \n",
    "        if output_format == \"pdb\":\n",
    "            return \",\".join([f\"A{i}\" for i in unique_sorted_ints])\n",
    "        else:\n",
    "            return \" \".join([str(i) for i in unique_sorted_ints])\n",
    "            \n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "# ==========================================\n",
    "# 1. Load & Preprocess fw_csv\n",
    "fw_csv = pd.read_csv('test/filter_cdr_idx.csv')\n",
    "fw_csv = fw_csv[['pdb_name', 'fw_index', 'r2_cdr_pos']].copy()\n",
    "fw_csv.rename(columns={'pdb_name': 'pdb_struc_name'}, inplace=True)\n",
    "\n",
    "# 2. Load & Preprocess key_res_csv\n",
    "key_res_csv = pd.read_csv(\"test/fixed_residue.csv\")\n",
    "output_path = \"test/fixed_residue_setting_for_partial_flow.csv\"\n",
    "\n",
    "key_res_csv[\"pdb_struc_name\"] = key_res_csv[\"pdb_name\"]#.apply(lambda x: '_'.join(str(x).split(\"_\")[:-1]))\n",
    "\n",
    "# 3. Merge DataFrames\n",
    "\n",
    "fixed_positions_df = fw_csv.merge(key_res_csv, on=\"pdb_struc_name\", how='inner')\n",
    "\n",
    "# 4. Extract Key Residues\n",
    "fixed_positions_df[\"key_res_index\"] = fixed_positions_df[\"key_res\"].apply(safe_extract_keys)\n",
    "\n",
    "# 5. Combine Indices (Raw String Concatenation)\n",
    "\n",
    "raw_combined_indices = (\n",
    "    fixed_positions_df['key_res_index'].fillna(\"\").astype(str) + \n",
    "    \" \" + \n",
    "    fixed_positions_df['fw_index'].fillna(\"\").astype(str)\n",
    ")\n",
    "\n",
    "# 6. Generate Final Columns\n",
    "\n",
    "fixed_positions_df['r2_fixed_sequence_pm'] = raw_combined_indices.apply(lambda x: process_indices(x, output_format=\"space\"))\n",
    "\n",
    "fixed_positions_df['r2_fixed_sequence'] = raw_combined_indices.apply(lambda x: process_indices(x, output_format=\"pdb\"))\n",
    "\n",
    "fixed_positions_df['r2_fixed_structure'] = fixed_positions_df['key_res_index'].apply(lambda x: process_indices(x, output_format=\"pdb\"))\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "fixed_positions_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data processed and saved to {output_path}\")\n",
    "print(fixed_positions_df[['r2_fixed_sequence_pm', 'r2_fixed_sequence']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a632d0",
   "metadata": {},
   "source": [
    "## Step.4.2 Partial Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc048c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status=0, jobnum:Submitted batch job 2910935\n",
      "status=0, jobnum:Submitted batch job 2910936\n",
      "status=0, jobnum:Submitted batch job 2910937\n",
      "status=0, jobnum:Submitted batch job 2910938\n"
     ]
    }
   ],
   "source": [
    "BASE='test'\n",
    "input_dir = f\"{BASE}/merge_motif_pdb\"\n",
    "output_dir = f\"{BASE}/pf\"\n",
    "hotspot=\"M28,M39,M55,M61\"\n",
    "shelldir = f\"{output_dir}/submit\"\n",
    "logdir = f'{output_dir}/log'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(shelldir, exist_ok=True)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "binder_chain, target_chain = [\"A\",\"C\"]\n",
    "samples_per_target=8\n",
    "t_start, t_end = [0.6,0.6]\n",
    "interface_dist = 10\n",
    "motif_contig = \"\"\n",
    "model_weights = \"path_to_ckpt\"\n",
    "\n",
    "fixed_positions_df = pd.read_csv(f\"{BASE}/fixed_residue_setting_for_partial_flow.csv\")\n",
    "for i, row in fixed_positions_df.iterrows():\n",
    "\n",
    "    pdb = os.path.join(input_dir, row[\"pdb_name\"]+\".pdb\")\n",
    "    pdb_name = row[\"pdb_name\"]\n",
    "    fixed_structure = row[\"r2_fixed_structure\"]\n",
    "    fixed_sequence = row[\"r2_fixed_sequence\"]\n",
    "    cdr_pos = row[\"r2_cdr_pos\"]\n",
    "\n",
    "    output_dir_temp = os.path.join(output_dir,row[\"pdb_name\"])\n",
    "\n",
    "    if os.path.exists(os.path.join(output_dir,pdb_name)):\n",
    "        continue\n",
    "    with open(f'{shelldir}/get_batch_{pdb_name}.sh', 'w') as f:\n",
    "        f.write(f\"#!/bin/bash \\n\"\n",
    "                f\"#SBATCH -p gpu41,gpu43 \\n\"\n",
    "                f\"#SBATCH --gres=gpu:1 \\n\"\n",
    "                f\"#SBATCH --ntasks=1 \\n\"\n",
    "                f\"#SBATCH --cpus-per-task=4\\n\"\n",
    "                f\"#SBATCH -N 1 \\n\"\n",
    "                f\"#SBATCH -o {logdir}/%j-{pdb_name}.out\\n\"\n",
    "                f\"#SBATCH -J {i}_partial\\n\"\n",
    "                f\"\\n\"\n",
    "                )\n",
    "        f.write(\n",
    "            'echo \"Job started at: $(date)\"\\n'\n",
    "            'start_time=$(date +%s)\\n'\n",
    "            'export LAYERNORM_TYPE=fast_layernorm \\n'\n",
    "            'export CUTLASS_PATH=cutlass \\n'\n",
    "            'export MPLBACKEND=Agg \\n'\n",
    "        )\n",
    "        cmd = (\"miniconda3/envs/fm56/bin/python sample_antibody_nanobody_partial.py \"\n",
    "                f\"--complex_pdb {pdb} \"\n",
    "                f\"--start_t {t_start} \"\n",
    "                f\"--fix_structure {fixed_structure} \"\n",
    "                f\"--fix_sequence {fixed_sequence} \"\n",
    "                f\"--antigen_chain M \"\n",
    "                f\"--heavy_chain A \"\n",
    "                f\"--config configs/inference_nanobody.yaml \"\n",
    "                f\"--samples_per_target {samples_per_target} \"\n",
    "                f\"--cdr_position {cdr_pos} \"\n",
    "                f\"--specified_hotspots '{hotspot}' \"\n",
    "                f\"--retry_Limit 10 \"\n",
    "                f\"--model_weights \\\"{model_weights}\\\" \"\n",
    "                f\"--output_dir \\\"{output_dir_temp}\\\" \" \n",
    "                f\"--name {pdb_name}\"\n",
    "            )\n",
    "\n",
    "        f.write(cmd)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\n",
    "            'end_time=$(date +%s)\\necho \"Job ended at: $(date)\"\\nelapsed=$((end_time - start_time))\\necho \"Elapsed time: $elapsed seconds\"')\n",
    "        \n",
    "    cmd = f\"sbatch {shelldir}/get_batch_{pdb_name}.sh\"\n",
    "    status, jobnum = subprocess.getstatusoutput(cmd)\n",
    "    print(f\"status={status}, jobnum:{jobnum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73530277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "src_root = Path(\"test/pf\")\n",
    "dst_dir = src_root / \"link_samples\"\n",
    "dst_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for subdir in src_root.iterdir():\n",
    "    if not subdir.is_dir():\n",
    "        continue\n",
    "    if subdir.name == \"link_samples\":\n",
    "        continue\n",
    "\n",
    "    for pdb in subdir.glob(\"sample*.pdb\"):\n",
    "\n",
    "        idx = pdb.stem.replace(\"sample\", \"\")\n",
    "\n",
    "        new_name = f\"{subdir.name}_{idx}.pdb\"\n",
    "        dst_path = dst_dir / new_name\n",
    "        if not dst_path.exists():\n",
    "            os.symlink(pdb.resolve(), dst_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70badc89",
   "metadata": {},
   "source": [
    "## Step.4.3 Sequence design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb06939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4dn4_ccl2_1\n",
      "status=0, jobnum:Submitted batch job 2910958\n",
      "4dn4_ccl2_8\n",
      "status=0, jobnum:Submitted batch job 2910959\n",
      "4dn4_ccl2_3\n",
      "status=0, jobnum:Submitted batch job 2910960\n",
      "4dn4_ccl2_0\n",
      "status=0, jobnum:Submitted batch job 2910961\n"
     ]
    }
   ],
   "source": [
    "# abmpnn \n",
    "input_dir = \"test/pf\"\n",
    "output_dir = \"test/fa\"\n",
    "resi = pd.read_csv('test/fixed_residue_setting_for_partial_flow.csv')\n",
    "PREFIX='ccl2'\n",
    "shelldir = f\"{output_dir}/submit\"\n",
    "logdir = f'{output_dir}/logs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(shelldir, exist_ok=True)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# Create the softlink directory if it doesn't exist\n",
    "num_seq_per_target=4\n",
    "sampling_temp=0.1\n",
    "batch_size=num_seq_per_target\n",
    "omit_AAs=\"C\"\n",
    "chains_to_design=\"A\"\n",
    "for i, folder in enumerate(os.listdir(input_dir)):\n",
    "    output_dir_temp = os.path.join(output_dir, folder)\n",
    "    if PREFIX not in folder or os.path.exists(output_dir_temp):\n",
    "        continue\n",
    "    else:\n",
    "        print(folder)\n",
    "        # pdb_name = \"_\".join(folder.split(\"_\")[:-1])\n",
    "        pdb_name = folder\n",
    "        folder_with_pdbs = os.path.join(input_dir, folder)\n",
    "        os.makedirs(output_dir_temp, exist_ok=True)\n",
    "        fixed_positions = resi.loc[resi['pdb_name'] == pdb_name, 'r2_fixed_sequence_pm'].values[0]\n",
    "\n",
    "        with open(f'{shelldir}/get_batch_{pdb_name}.sh', 'w') as f:\n",
    "            f.write(f\"#!/bin/bash \\n\"\n",
    "                    f\"#SBATCH -p gpu41,gpu43 \\n\"\n",
    "                    f\"#SBATCH --gres=gpu:1 \\n\"\n",
    "                    f\"#SBATCH --ntasks=1 \\n\"\n",
    "                    f\"#SBATCH --cpus-per-task=1\\n\"\n",
    "                    f\"#SBATCH -N 1 \\n\"\n",
    "                    f\"#SBATCH -o {logdir}/%j-{pdb_name}.out\\n\"\n",
    "                    f\"#SBATCH -J {i}_proteinmpnn\\n\"\n",
    "                    f\"\\n\"\n",
    "                    )\n",
    "            f.write(\n",
    "                'echo \"Job started at: $(date)\"\\n'\n",
    "                'start_time=$(date +%s)\\n'\n",
    "                'export WANDB_MODE=disabled\\n'\n",
    "                'source miniconda3/etc/profile.d/conda.sh \\n'\n",
    "                'conda activate mlfold \\n'\n",
    "            )\n",
    "\n",
    "            cmd = ( f\"folder_with_pdbs={folder_with_pdbs} \\n\"\n",
    "                    f'fixed_positions=\"{fixed_positions}\" \\n'\n",
    "                    f\"output_dir={output_dir_temp} \\n\\n\"\n",
    "                    f\"path_for_parsed_chains={os.path.join(output_dir_temp,'parsed_pdbs.jsonl')} \\n\"\n",
    "                    f\"path_for_assigned_chains={os.path.join(output_dir_temp,'assigned_pdbs.jsonl')} \\n\\n\"\n",
    "                    f\"path_for_fixed_positions={os.path.join(output_dir_temp,'fixed_pdbs.jsonl')} \\n\\n\"\n",
    "                    f\"python ProteinMPNN-main/helper_scripts/parse_multiple_chains.py --input_path={folder_with_pdbs} --output_path=$path_for_parsed_chains \\n\"\n",
    "                    f\"python ProteinMPNN-main/helper_scripts/assign_fixed_chains.py --input_path=$path_for_parsed_chains --output_path=$path_for_assigned_chains --chain_list {chains_to_design} \\n\\n\"\n",
    "                    f\"python ProteinMPNN-main/helper_scripts/make_fixed_positions_dict.py --input_path=$path_for_parsed_chains --output_path=$path_for_fixed_positions --chain_list {chains_to_design} --position_list '{fixed_positions}' \\n\"\n",
    "                    f\"python ProteinMPNN-main/protein_mpnn_run.py \\\\\\n\"\n",
    "                    f\"--path_to_model_weights ProteinMPNN-main/model_weights \\\\\\n\"\n",
    "                    f\"--model_name 'abmpnn' \\\\\\n\"\n",
    "                    f\"--jsonl_path $path_for_parsed_chains \\\\\\n\"\n",
    "                    f\"--chain_id_jsonl $path_for_assigned_chains \\\\\\n\"\n",
    "                    f\"--fixed_positions_jsonl $path_for_fixed_positions \\\\\\n\"\n",
    "                    f\"--out_folder $output_dir \\\\\\n\"\n",
    "                    f\"--num_seq_per_target {num_seq_per_target} \\\\\\n\"\n",
    "                    f\"--sampling_temp {sampling_temp} \\\\\\n\"\n",
    "                    f\"--batch_size {batch_size} \\\\\\n\"\n",
    "                    f\"--use_soluble_model \\\\\\n\"\n",
    "                    f\"--omit_AAs {omit_AAs} \\n\\n\"\n",
    "                    )\n",
    "\n",
    "            # print(cmd)\n",
    "            f.write(cmd)\n",
    "            f.write(\n",
    "                'end_time=$(date +%s)\\necho \"Job ended at: $(date)\"\\nelapsed=$((end_time - start_time))\\necho \"Elapsed time: $elapsed seconds\"')\n",
    "                \n",
    "        cmd = f\"sbatch {shelldir}/get_batch_{pdb_name}.sh\"\n",
    "        status, jobnum = subprocess.getstatusoutput(cmd)\n",
    "        print(f\"status={status}, jobnum:{jobnum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915887d",
   "metadata": {},
   "source": [
    "# Step.5 Scoring & Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f0fa810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 files. Starting parallel parsing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fb3ffeccde4290b108e2bcf904a385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing FASTA files in parallel:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All files have been parsed. Creating DataFrame and saving to CSV...\n",
      "Successfully saved 160 sequences to test/fa.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def _parse_single_fasta(fasta_path: Path) -> list:\n",
    "\n",
    "    records = []\n",
    "    path_str = str(fasta_path)\n",
    "    try:\n",
    "        with fasta_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "            seq_idx = -1\n",
    "            seq_buf = []\n",
    "\n",
    "            for line in fh:\n",
    "                if line.startswith(\">\"):\n",
    "                    if seq_buf:\n",
    "                        records.append([path_str, \"\".join(seq_buf), seq_idx])\n",
    "                        seq_buf = []\n",
    "                    seq_idx += 1\n",
    "                else:\n",
    "\n",
    "                    stripped_line = line.strip()\n",
    "                    if stripped_line:\n",
    "                        seq_buf.append(stripped_line)\n",
    "\n",
    "            if seq_buf:\n",
    "                records.append([path_str, \"\".join(seq_buf), seq_idx])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipping file {fasta_path} due to error: {e}\")\n",
    "        return []\n",
    "\n",
    "    return records\n",
    "\n",
    "def read_all_sequences_parallel(input_dir: str, output_csv: str) -> pd.DataFrame:\n",
    "    fasta_paths = list(Path(input_dir).rglob(\"*.fa\"))\n",
    "    if not fasta_paths:\n",
    "        print(\"Warning: No .fa files found in the specified directory.\")\n",
    "        df = pd.DataFrame(columns=[\"fasta\", \"seq\", \"seq_idx\"])\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        return df\n",
    "    \n",
    "    print(f\"Found {len(fasta_paths)} files. Starting parallel parsing...\")\n",
    "\n",
    "    num_processes = cpu_count()\n",
    "    all_records = []\n",
    "\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        with tqdm(total=len(fasta_paths), desc=\"Parsing FASTA files in parallel\") as pbar:\n",
    "            for records_from_one_file in pool.imap_unordered(_parse_single_fasta, fasta_paths):\n",
    "                if records_from_one_file:\n",
    "                    all_records.extend(records_from_one_file)\n",
    "                pbar.update(1)\n",
    "\n",
    "    if not all_records:\n",
    "        print(\"Warning: All files are empty or could not be parsed; the resulting CSV will be empty.\")\n",
    "\n",
    "    print(\"\\nAll files have been parsed. Creating DataFrame and saving to CSV...\")\n",
    "    df = pd.DataFrame(all_records, columns=[\"fasta\", \"seq\", \"seq_idx\"])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Successfully saved {len(df)} sequences to {output_csv}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "input_directory = \"test/fa\"\n",
    "output_csv = \"test/fa.csv\"\n",
    "sequences_df = read_all_sequences_parallel(input_directory, output_csv)\n",
    "sequences_df['seq_idx']= '' + sequences_df['seq_idx'].astype(str)\n",
    "sequences_df.to_csv(output_csv,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7523c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_name</th>\n",
       "      <th>seq</th>\n",
       "      <th>seq_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4dn4_ccl2_3_7.pdb</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGGSMSSYAMAWYRQAPGKGRE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4dn4_ccl2_3_7.pdb</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGGSMSSYAMAWYRQAPGKGRE...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4dn4_ccl2_3_7.pdb</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGGTMSSYAMAWYRQAPGKGRE...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4dn4_ccl2_3_7.pdb</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGGSMSSYAMAWYRQAPGKGRE...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4dn4_ccl2_1_4.pdb</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGGNISSSYMAWYRQAPGKGRE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>4dn4_ccl2_1_7.pdb</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGGSVSDSTMAWYRQAPGKGRE...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>4dn4_ccl2_1_5.pdb</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGGTFSSSSMAWYRQAPGKGRE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>4dn4_ccl2_1_5.pdb</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGGTFSSSSMAWYRQAPGKGRE...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>4dn4_ccl2_1_5.pdb</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGGTFSSSSMAWYRQAPGKGRE...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>4dn4_ccl2_1_5.pdb</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGGTFSSSSMAWYRQAPGKGRE...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             link_name                                                seq  \\\n",
       "1    4dn4_ccl2_3_7.pdb  EVQLVESGGGLVQPGGSLRLSCAASGGSMSSYAMAWYRQAPGKGRE...   \n",
       "2    4dn4_ccl2_3_7.pdb  EVQLVESGGGLVQPGGSLRLSCAASGGSMSSYAMAWYRQAPGKGRE...   \n",
       "3    4dn4_ccl2_3_7.pdb  EVQLVESGGGLVQPGGSLRLSCAASGGTMSSYAMAWYRQAPGKGRE...   \n",
       "4    4dn4_ccl2_3_7.pdb  EVQLVESGGGLVQPGGSLRLSCAASGGSMSSYAMAWYRQAPGKGRE...   \n",
       "6    4dn4_ccl2_1_4.pdb  EVQLVESGGGLVQPGGSLRLSCAASGGNISSSYMAWYRQAPGKGRE...   \n",
       "..                 ...                                                ...   \n",
       "154  4dn4_ccl2_1_7.pdb  EVQLVESGGGLVQPGGSLRLSCAASGGSVSDSTMAWYRQAPGKGRE...   \n",
       "156  4dn4_ccl2_1_5.pdb  EVQLVESGGGLVQPGGSLRLSCAASGGTFSSSSMAWYRQAPGKGRE...   \n",
       "157  4dn4_ccl2_1_5.pdb  EVQLVESGGGLVQPGGSLRLSCAASGGTFSSSSMAWYRQAPGKGRE...   \n",
       "158  4dn4_ccl2_1_5.pdb  EVQLVESGGGLVQPGGSLRLSCAASGGTFSSSSMAWYRQAPGKGRE...   \n",
       "159  4dn4_ccl2_1_5.pdb  EVQLVESGGGLVQPGGSLRLSCAASGGTFSSSSMAWYRQAPGKGRE...   \n",
       "\n",
       "     seq_idx  \n",
       "1          1  \n",
       "2          2  \n",
       "3          3  \n",
       "4          4  \n",
       "6          1  \n",
       "..       ...  \n",
       "154        4  \n",
       "156        1  \n",
       "157        2  \n",
       "158        3  \n",
       "159        4  \n",
       "\n",
       "[128 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('test/fa.csv')\n",
    "df = df[\n",
    "    ~df[\"seq_idx\"].isin(['0',0]) # remove wt\n",
    "]\n",
    "df['orig_path'] = df['fasta']\n",
    "df['orig_path'] = (df['orig_path']\n",
    "                   .str.replace('test', 'test', regex=False)\n",
    "                   .str.replace('/fa', '/pf/linked_samples', regex=True)\n",
    "                   .str.replace('/seqs/sample', '_', regex=False)\n",
    "                   .str.replace('.fa', '.pdb', regex=False)\n",
    "                  )\n",
    "\n",
    "df['link_name'] = df['orig_path'].str.split('/').str[-1]\n",
    "df = df[['link_name','seq','seq_idx']]\n",
    "df.to_csv('test/pf_fa_sum.csv',index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f60ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run in a tmux terminal\n",
    "```\n",
    "bash demo_scripts/flowpacker_af3score.sh\\\n",
    "    test/pf/link_samples \\\n",
    "    test/pf_fa_sum.csv \\\n",
    "    test/af3score_r2 \\\n",
    "    3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5644425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os,shutil\n",
    "df=pd.read_csv('test/af3score_r2/af3score_base_outputs/af3score_metrics.csv')\n",
    "df['pdb_path']='test/af3score_r2/flowpacker/flowpacker_outputs/run_1/'+df['description']+'.pdb'\n",
    "df_f=df[(df['iptm'] > 0.5)&(df['ptm_A'] > 0.8)]\n",
    "print(df_f.shape)\n",
    "out_dir = f'test/af3score_r2/filtered_links'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "df_f.apply(lambda row: os.link(os.path.abspath(row['pdb_path']), \n",
    "                                  os.path.join(out_dir, row['description']+'.pdb')), axis=1)\n",
    "print(f\"Linked {len(df_f)} files to {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be39ae86",
   "metadata": {},
   "source": [
    "#### Note for Testing\n",
    "In some cases, you may find that **no designs pass the filter criteria**. \n",
    "\n",
    "To address this, you can **increase the number of sampled designs** or **adjust the hotspots**. \n",
    "\n",
    "A few pilot runs are recommended to optimize these settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39db12d",
   "metadata": {},
   "source": [
    "# Steq.6 Rosetta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python demo_scripts/submit_relax_comp.py test/af3score_r2/filtered_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be98e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'test/af3score_r2/filtered_links/rst_itf_nofix'\n",
    "\n",
    "all_data = pd.DataFrame()\n",
    "csv_files = glob.glob(os.path.join(csv_path, 'rosetta_complex*.csv'))\n",
    "\n",
    "for csv in csv_files:\n",
    "    data = pd.read_csv(csv)\n",
    "    all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "output_csv_path = os.path.join(f'test/all_rst_comp.csv') # interface score\n",
    "all_data.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1862d",
   "metadata": {},
   "source": [
    "# Steq.7 AF3 Refolding\n",
    "\n",
    "Refolding is performed **without a template**.  \n",
    "We **recommend using 20 seeds × 5 models** to improve prediction quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c9f03",
   "metadata": {},
   "source": [
    "## dockq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fabb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python demo_scripts/run_DockQv2_eachfolder_v2.py \\\n",
    "  --input_dir path_to_af3_out \\\n",
    "  --reference_dir test/af3score_r2/filtered_links \\\n",
    "  --output_dir test/dockq \\\n",
    "  --shell_dir test/dockq_shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python demo_scripts/HNMT/parse_dockq_scores.py test/dockq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24865b",
   "metadata": {},
   "source": [
    "# Final Filtering Criteria\n",
    "\n",
    "### Structural Quality Thresholds\n",
    "The following filters are applied to retain high-confidence models:\n",
    "\n",
    "- **DockQ > 0.49**\n",
    "- **AF3 pTM > 0.8**\n",
    "- **AF3 ipTM > 0.7**\n",
    "\n",
    "### Ranking Score\n",
    "Candidates are ranked using the following empirical score:\n",
    "```Score = (AF3 ipTM × 100) − Interface Score```\n",
    "\n",
    "### Notes\n",
    "- This score is **only meaningful for ranking candidates within the same target and the same molecular type**.\n",
    "- It does **not** have an absolute physical or biological interpretation.\n",
    "- The purpose is **relative prioritization**, not cross-target comparison.\n",
    "\n",
    "Good luck.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
